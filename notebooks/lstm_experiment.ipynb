{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01ba8b61",
   "metadata": {},
   "source": [
    "# FWI prediction (LSTM)\n",
    "\n",
    "This notebook is a **FWI-only** version: it trains an LSTM on historical FWI GeoTIFFs and forecasts the next timestep grid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed458cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e6ac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import rasterio\n",
    "from rasterio.warp import reproject, Resampling\n",
    "from rasterio.io import DatasetReader\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a39b16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR=\"Jin_fwi2\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Choose a resampling for continuous meteorological fields:\n",
    "RESAMPLE_METHOD = Resampling.bilinear   # use Resampling.nearest for categorical fields\n",
    "\n",
    "# Weâ€™ll standardize nodata to float32 NaN in outputs\n",
    "DST_NODATA = np.nan\n",
    "DST_DTYPE = \"float32\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79fda3c",
   "metadata": {},
   "source": [
    "## FWI raster utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a17f038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import os\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import zipfile\n",
    "\n",
    "\n",
    "def unzip_fwi_zip(zip_path: str, out_dir: str) -> str:\n",
    "    \"\"\"Unzip a FWI GeoTIFF archive into out_dir. Returns out_dir.\"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        z.extractall(out_dir)\n",
    "    print(\"Unzipped to:\", out_dir)\n",
    "    return out_dir\n",
    "\n",
    "\n",
    "\n",
    "def _parse_date_from_path(path: str) -> Optional[str]:\n",
    "    \"\"\"Try to extract YYYYMMDD or YYYYMMDDTHHMM from a filename.\"\"\"\n",
    "    m = re.search(r'(\\d{8}T\\d{4})', path)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    m = re.search(r'(\\d{8})', path)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    return None\n",
    "\n",
    "\n",
    "def load_single_raster(path: str, band: int = 1, to_float32: bool = True) -> Tuple[np.ndarray, dict]:\n",
    "    \"\"\"Load a GeoTIFF band to a numpy array, converting nodata to NaN.\"\"\"\n",
    "    with rasterio.open(path) as ds:\n",
    "        arr = ds.read(band)\n",
    "        profile = ds.profile.copy()\n",
    "        nodata = ds.nodata\n",
    "\n",
    "    if to_float32:\n",
    "        arr = arr.astype(np.float32, copy=False)\n",
    "\n",
    "    if nodata is not None and not (isinstance(nodata, float) and np.isnan(nodata)):\n",
    "        arr = np.where(arr == np.float32(nodata), np.nan, arr)\n",
    "\n",
    "    return arr, profile\n",
    "\n",
    "\n",
    "def load_raster_stack(paths: List[str], band: int = 1) -> Tuple[np.ndarray, List[str], dict]:\n",
    "    \"\"\"\n",
    "    Load rasters into a stack of shape [T, H, W].\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    paths : list[str]\n",
    "        GeoTIFF paths.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    stack : np.ndarray\n",
    "        float32 with NaN nodata, shape [T, H, W]\n",
    "    time_keys : list[str]\n",
    "        Parsed date/time keys (best-effort) aligned with T.\n",
    "    ref_profile : dict\n",
    "        Profile from the first raster (useful for writing outputs).\n",
    "    \"\"\"\n",
    "    if len(paths) == 0:\n",
    "        raise ValueError('paths is empty')\n",
    "\n",
    "    # Sort by extracted time key if possible\n",
    "    keyed = []\n",
    "    for p in paths:\n",
    "        k = _parse_date_from_path(p) or p\n",
    "        keyed.append((k, p))\n",
    "    keyed.sort(key=lambda x: x[0])\n",
    "\n",
    "    time_keys = [k for k, _ in keyed]\n",
    "    sorted_paths = [p for _, p in keyed]\n",
    "\n",
    "    first, ref_profile = load_single_raster(sorted_paths[0], band=band)\n",
    "    T = len(sorted_paths)\n",
    "    H, W = first.shape\n",
    "\n",
    "    stack = np.empty((T, H, W), dtype=np.float32)\n",
    "    stack[0] = first\n",
    "\n",
    "    for i, p in enumerate(sorted_paths[1:], start=1):\n",
    "        a, prof = load_single_raster(p, band=band)\n",
    "        if a.shape != (H, W):\n",
    "            raise ValueError(f'Shape mismatch for {p}: got {a.shape}, expected {(H, W)}')\n",
    "        stack[i] = a\n",
    "\n",
    "    return stack, time_keys, ref_profile\n",
    "\n",
    "\n",
    "def fit_standard_scaler(x: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Fit a simple per-feature standard scaler on x with shape [N, L, F].\"\"\"\n",
    "    flat = x.reshape(-1, x.shape[-1])\n",
    "    mean = np.nanmean(flat, axis=0)\n",
    "    std = np.nanstd(flat, axis=0)\n",
    "    std = np.where(std < 1e-6, 1.0, std)\n",
    "    return mean.astype(np.float32), std.astype(np.float32)\n",
    "\n",
    "\n",
    "def apply_standard_scaler(x: np.ndarray, mean: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
    "    return ((x - mean) / std).astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "def sample_sequences(\n",
    "    y_stack: np.ndarray,\n",
    "    x_stack: Optional[np.ndarray] = None,\n",
    "    lookback: int = 14,\n",
    "    horizon: int = 1,\n",
    "    n_samples: int = 200_000,\n",
    "    seed: int = 42,\n",
    "    require_finite: bool = True,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Randomly sample training sequences from rasters.\n",
    "\n",
    "    y_stack: [T,H,W] target FWI.\n",
    "    x_stack: optional [T,H,W,F] features aligned to y_stack timesteps.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X: [N, lookback, n_features]\n",
    "    y: [N, horizon]\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - If x_stack is provided, per-timestep features become [FWI, x1..xF] by default.\n",
    "    - NaNs are treated as missing; windows containing NaNs are skipped.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    T, H, W = y_stack.shape\n",
    "    if T < lookback + horizon:\n",
    "        raise ValueError('Not enough timesteps for given lookback+horizon')\n",
    "\n",
    "    F = 0 if x_stack is None else int(x_stack.shape[-1])\n",
    "    n_features = 1 + F\n",
    "\n",
    "    X = np.empty((n_samples, lookback, n_features), dtype=np.float32)\n",
    "    y = np.empty((n_samples, horizon), dtype=np.float32)\n",
    "\n",
    "    max_t = T - horizon\n",
    "    filled = 0\n",
    "    max_tries = n_samples * 30\n",
    "\n",
    "    for _ in range(max_tries):\n",
    "        if filled >= n_samples:\n",
    "            break\n",
    "\n",
    "        t = int(rng.integers(lookback, max_t))\n",
    "        r = int(rng.integers(0, H))\n",
    "        c = int(rng.integers(0, W))\n",
    "\n",
    "        y_hist = y_stack[t - lookback:t, r, c]\n",
    "        y_fut = y_stack[t:t + horizon, r, c]\n",
    "\n",
    "        if require_finite and (np.any(~np.isfinite(y_hist)) or np.any(~np.isfinite(y_fut))):\n",
    "            continue\n",
    "\n",
    "        if x_stack is not None:\n",
    "            x_hist = x_stack[t - lookback:t, r, c, :]  # [L,F]\n",
    "            if require_finite and np.any(~np.isfinite(x_hist)):\n",
    "                continue\n",
    "            feat = np.concatenate([y_hist[:, None], x_hist], axis=1)\n",
    "        else:\n",
    "            feat = y_hist[:, None]\n",
    "\n",
    "        X[filled] = feat\n",
    "        y[filled] = y_fut\n",
    "        filled += 1\n",
    "\n",
    "    if filled == 0:\n",
    "        raise RuntimeError('Could not sample any valid sequences (too many NaNs / no overlap).')\n",
    "\n",
    "    return X[:filled], y[:filled]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb06ade",
   "metadata": {},
   "source": [
    "## LSTM model + training utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a818327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class LSTMForecaster(nn.Module):\n",
    "    \"\"\"Many-to-one LSTM that predicts `horizon` future values from the last hidden state.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features: int,\n",
    "        hidden_size: int = 64,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "        horizon: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.horizon = horizon\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_features,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, horizon),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [B, L, F]\n",
    "        out, _ = self.lstm(x)\n",
    "        last = out[:, -1, :]  # [B, H]\n",
    "        return self.head(last)\n",
    "\n",
    "\n",
    "\n",
    "class TrainConfig:\n",
    "    epochs: int = 20\n",
    "    batch_size: int = 4096\n",
    "    lr: float = 1e-3\n",
    "    weight_decay: float = 1e-5\n",
    "    patience: int = 5\n",
    "    num_workers: int = 0\n",
    "\n",
    "\n",
    "def _rmse(pred: torch.Tensor, targ: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.sqrt(torch.mean((pred - targ) ** 2))\n",
    "\n",
    "\n",
    "def train_lstm(\n",
    "    model: nn.Module,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_val: np.ndarray,\n",
    "    y_val: np.ndarray,\n",
    "    cfg: TrainConfig = TrainConfig(),\n",
    "    device: Optional[str] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Train with early stopping on validation RMSE.\"\"\"\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        SequenceDataset(X_train, y_train),\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=(device == 'cuda'),\n",
    "        drop_last=False,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        SequenceDataset(X_val, y_val),\n",
    "        batch_size=cfg.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.num_workers,\n",
    "        pin_memory=(device == 'cuda'),\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    best_val = float('inf')\n",
    "    best_state = None\n",
    "    bad = 0\n",
    "    history = {'train_rmse': [], 'val_rmse': []}\n",
    "\n",
    "    for epoch in range(1, cfg.epochs + 1):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "            train_losses.append(_rmse(pred.detach(), yb.detach()).item())\n",
    "\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device, non_blocking=True)\n",
    "                yb = yb.to(device, non_blocking=True)\n",
    "                pred = model(xb)\n",
    "                val_losses.append(_rmse(pred, yb).item())\n",
    "\n",
    "        tr = float(np.mean(train_losses))\n",
    "        va = float(np.mean(val_losses))\n",
    "        history['train_rmse'].append(tr)\n",
    "        history['val_rmse'].append(va)\n",
    "        print(f\"Epoch {epoch:02d}/{cfg.epochs} | train RMSE={tr:.4f} | val RMSE={va:.4f}\")\n",
    "\n",
    "        if va < best_val:\n",
    "            best_val = va\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= cfg.patience:\n",
    "                print(f\"Early stopping (best val RMSE={best_val:.4f})\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return {'best_val_rmse': best_val, 'history': history, 'device': device}\n",
    "\n",
    "\n",
    "def predict_lstm(\n",
    "    model: nn.Module,\n",
    "    X: np.ndarray,\n",
    "    batch_size: int = 8192,\n",
    "    device: Optional[str] = None,\n",
    ") -> np.ndarray:\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    loader = DataLoader(\n",
    "        SequenceDataset(X, np.zeros((X.shape[0], 1), dtype=np.float32)),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=(device == 'cuda'),\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    outs = []\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in loader:\n",
    "            xb = xb.to(device, non_blocking=True)\n",
    "            outs.append(model(xb).detach().cpu().numpy())\n",
    "\n",
    "    return np.concatenate(outs, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bde907f",
   "metadata": {},
   "source": [
    "## Grid inference + GeoTIFF writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970172ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_grid_lstm(\n",
    "    model: LSTMForecaster,\n",
    "    y_stack: np.ndarray,\n",
    "    x_stack: Optional[np.ndarray],\n",
    "    lookback: int,\n",
    "    mean: np.ndarray,\n",
    "    std: np.ndarray,\n",
    "    batch_pixels: int = 65536,\n",
    "    device: Optional[str] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Predict the next-horizon FWI grid from the last `lookback` timesteps.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pred : np.ndarray\n",
    "        Shape [H, W] if horizon=1, else [horizon, H, W]\n",
    "        (NaN where the input window contains missing values).\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    T, H, W = y_stack.shape\n",
    "    if T < lookback:\n",
    "        raise ValueError('Not enough timesteps for inference lookback')\n",
    "\n",
    "    y_win = y_stack[T - lookback:T]  # [L,H,W]\n",
    "\n",
    "    F = 0 if x_stack is None else int(x_stack.shape[-1])\n",
    "    horizon = int(model.horizon)\n",
    "\n",
    "    # Prepare output\n",
    "    if horizon == 1:\n",
    "        out = np.full((H, W), np.nan, dtype=np.float32)\n",
    "    else:\n",
    "        out = np.full((horizon, H, W), np.nan, dtype=np.float32)\n",
    "\n",
    "    # Flatten pixel dimension (do NOT materialize giant [P,L,F] arrays)\n",
    "    P = H * W\n",
    "    y_flat = y_win.reshape(lookback, P)  # view if contiguous\n",
    "\n",
    "    if x_stack is not None:\n",
    "        x_win = x_stack[T - lookback:T]  # [L,H,W,F]\n",
    "        x_flat = x_win.reshape(lookback, P, F)\n",
    "\n",
    "    for start in range(0, P, batch_pixels):\n",
    "        end = min(P, start + batch_pixels)\n",
    "        yb = y_flat[:, start:end].T  # [B, L]\n",
    "\n",
    "        if x_stack is not None:\n",
    "            xb = x_flat[:, start:end, :].transpose(1, 0, 2)  # [B, L, F]\n",
    "            feat = np.concatenate([yb[:, :, None], xb], axis=2)\n",
    "        else:\n",
    "            feat = yb[:, :, None]\n",
    "\n",
    "        # valid where no NaNs\n",
    "        valid = np.all(np.isfinite(feat), axis=(1, 2))\n",
    "        if not np.any(valid):\n",
    "            continue\n",
    "\n",
    "        feat_valid = feat[valid]\n",
    "        feat_valid = apply_standard_scaler(feat_valid, mean, std)\n",
    "\n",
    "        # predict\n",
    "        pred = predict_lstm(model, feat_valid, batch_size=8192, device=device)  # [Nv, horizon]\n",
    "\n",
    "        # scatter back\n",
    "        idx = np.where(valid)[0] + start\n",
    "        if horizon == 1:\n",
    "            out.reshape(-1)[idx] = pred[:, 0].astype(np.float32)\n",
    "        else:\n",
    "            for h in range(horizon):\n",
    "                out[h].reshape(-1)[idx] = pred[:, h].astype(np.float32)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def write_geotiff(path: str, arr: np.ndarray, ref_profile: dict):\n",
    "    \"\"\"Write [H,W] (or [1,H,W]) float32 with NaN nodata using ref_profile.\"\"\"\n",
    "    import rasterio\n",
    "\n",
    "    profile = ref_profile.copy()\n",
    "    profile.update(dtype='float32', nodata=np.nan, compress='deflate', predictor=3, BIGTIFF='IF_SAFER')\n",
    "\n",
    "    if arr.ndim == 2:\n",
    "        profile.update(count=1)\n",
    "        data = arr[None, ...]\n",
    "    elif arr.ndim == 3 and arr.shape[0] >= 1:\n",
    "        profile.update(count=arr.shape[0])\n",
    "        data = arr\n",
    "    else:\n",
    "        raise ValueError('arr must be [H,W] or [B,H,W]')\n",
    "\n",
    "    with rasterio.open(path, 'w', **profile) as dst:\n",
    "        dst.write(data.astype(np.float32), indexes=list(range(1, data.shape[0] + 1)))\n",
    "\n",
    "    print('Wrote:', path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa0eaa1",
   "metadata": {},
   "source": [
    "## FWI-only training + forecast wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08549aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "\n",
    "def run_lstm_fwi_forecast(\n",
    "    fwi_paths: List[str],\n",
    "    lookback: int = 14,\n",
    "    horizon: int = 1,\n",
    "    n_samples: int = 200000,\n",
    "    val_frac: float = 0.1,\n",
    "    seed: int = 42,\n",
    "    model_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    train_cfg: TrainConfig = TrainConfig(),\n",
    "    device: Optional[str] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Train an LSTM forecaster on past FWI only and produce a next-horizon FWI grid forecast.\n",
    "\n",
    "    Returns a dict with keys:\n",
    "      - model, train_info\n",
    "      - pred_grid (next horizon forecast)\n",
    "      - ref_profile (for write_geotiff)\n",
    "      - mean, std (scaler)\n",
    "      - y_keys (time keys)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Uses random sampling of (pixel, time) windows to keep training practical on large grids.\n",
    "    - Windows containing NaNs are skipped.\n",
    "    \"\"\"\n",
    "    # Load target\n",
    "    y_stack, y_keys, ref_profile = load_raster_stack(fwi_paths)\n",
    "\n",
    "    # Sample sequences\n",
    "    X, y = sample_sequences(\n",
    "        y_stack=y_stack,\n",
    "        x_stack=None,\n",
    "        lookback=lookback,\n",
    "        horizon=horizon,\n",
    "        n_samples=n_samples,\n",
    "        seed=seed,\n",
    "        require_finite=True,\n",
    "    )\n",
    "\n",
    "    # Shuffle + split\n",
    "    rng = np.random.default_rng(seed)\n",
    "    order = rng.permutation(X.shape[0])\n",
    "    X = X[order]\n",
    "    y = y[order]\n",
    "\n",
    "    n_val = max(1, int(val_frac * X.shape[0]))\n",
    "    X_val, y_val = X[:n_val], y[:n_val]\n",
    "    X_train, y_train = X[n_val:], y[n_val:]\n",
    "\n",
    "    # Scale\n",
    "    mean, std = fit_standard_scaler(X_train)\n",
    "    X_train_s = apply_standard_scaler(X_train, mean, std)\n",
    "    X_val_s = apply_standard_scaler(X_val, mean, std)\n",
    "\n",
    "    # Model\n",
    "    model_kwargs = model_kwargs or {}\n",
    "    model = LSTMForecaster(n_features=X.shape[-1], horizon=horizon, **model_kwargs)\n",
    "    train_info = train_lstm(\n",
    "        model=model,\n",
    "        X_train=X_train_s,\n",
    "        y_train=y_train,\n",
    "        X_val=X_val_s,\n",
    "        y_val=y_val,\n",
    "        cfg=train_cfg,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Forecast next grid\n",
    "    pred_grid = predict_next_grid_lstm(\n",
    "        model=model,\n",
    "        y_stack=y_stack,\n",
    "        x_stack=None,\n",
    "        lookback=lookback,\n",
    "        mean=mean,\n",
    "        std=std,\n",
    "        batch_pixels=65536,\n",
    "        device=train_info.get('device', device),\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'train_info': train_info,\n",
    "        'pred_grid': pred_grid,\n",
    "        'ref_profile': ref_profile,\n",
    "        'mean': mean,\n",
    "        'std': std,\n",
    "        'y_keys': y_keys,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47d2f92",
   "metadata": {},
   "source": [
    "## End-to-end run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8d3f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# --- Data location ---\n",
    "# If you uploaded a ZIP of daily FWI GeoTIFFs, set ZIP_PATH and unzip to FWI_DIR.\n",
    "# Otherwise, set FWI_DIR to the folder that already contains your .tif files.\n",
    "ZIP_PATH = 'fwi_tifs_2025_09_12_2025_10_10.zip'  # adjust\n",
    "FWI_DIR  = 'fwi'                                # adjust\n",
    "\n",
    "if os.path.exists(ZIP_PATH):\n",
    "    unzip_fwi_zip(ZIP_PATH, FWI_DIR)\n",
    "\n",
    "# --- Collect FWI files ---\n",
    "fwi_paths = sorted(glob.glob(os.path.join(FWI_DIR, '**', '*.tif'), recursive=True))\n",
    "print('FWI tif count:', len(fwi_paths))\n",
    "if len(fwi_paths) == 0:\n",
    "    raise FileNotFoundError('No .tif files found under FWI_DIR. Check ZIP_PATH/FWI_DIR.')\n",
    "\n",
    "# --- Load time series stack ---\n",
    "y_stack, y_keys, ref_profile = load_raster_stack(fwi_paths)\n",
    "print('Loaded y_stack:', y_stack.shape, 'timesteps:', len(y_keys))\n",
    "\n",
    "# --- Train + forecast ---\n",
    "result = run_lstm_fwi_forecast(\n",
    "    fwi_paths=fwi_paths,\n",
    "    lookback=14,\n",
    "    horizon=1,\n",
    "    n_samples=200_000,\n",
    "    val_frac=0.1,\n",
    "    seed=42,\n",
    "    model_kwargs={'hidden_size': 128, 'num_layers': 2, 'dropout': 0.1},\n",
    "    train_cfg=TrainConfig(epochs=10, batch_size=4096, lr=1e-3, patience=3),\n",
    ")\n",
    "\n",
    "pred = result['pred_grid']\n",
    "print('pred_grid shape:', pred.shape)\n",
    "\n",
    "# --- Write output ---\n",
    "out_path = os.path.join(OUT_DIR, 'fwi_lstm_forecast.tif')\n",
    "write_geotiff(out_path, pred if pred.ndim == 2 else pred[0], result['ref_profile'])\n",
    "print('Wrote forecast:', out_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
